{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LakhiCharanMahato/Image_to_Emotion/blob/main/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfyK0tzQGIup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d325c77-89fa-4d36-8fee-90ac8d24d5c2"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!pip install --upgrade kaggle\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 601 ~/.kaggle/kaggle.json\n",
        "#https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews -p /content\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip\n",
        "import pandas as pd\n",
        "df=pd.read_csv('IMDB Dataset.csv')\n",
        "#train dataset\n",
        "train_reviews=df.review[:40000]\n",
        "train_sentiments=df.sentiment[:40000]\n",
        "#test dataset\n",
        "test_reviews=df.review[40000:]\n",
        "test_sentiments=df.sentiment[40000:]\n",
        "print(train_reviews.shape,train_sentiments.shape)\n",
        "print(test_reviews.shape,test_sentiments.shape)\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 35% 9.00M/25.7M [00:00<00:00, 70.4MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 102MB/s] \n",
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n",
            "(40000,) (40000,)\n",
            "(10000,) (10000,)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JniVIh0KGYeZ"
      },
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly-cv5reGZwv"
      },
      "source": [
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "from string import punctuation\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    text=''.join([c for c in text if c not in punctuation])\n",
        "    text=denoise_text(text)\n",
        "    text=simple_stemmer(text)\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2S-Q982GeNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd22ee36-5299-437f-eb2f-9d93e0d8693c"
      },
      "source": [
        "%%time\n",
        "import re\n",
        "df.review = df.review.apply(lambda x: preprocess(x))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 37s, sys: 831 ms, total: 4min 38s\n",
            "Wall time: 4min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc40dVAtGlwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8719aa1d-1bac-4c28-9bfe-f66f382f5c51"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TRAIN_SIZE = 0.80\n",
        "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN size: 40000\n",
            "TEST size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFLNV-gqGpQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3eb4600-6f9d-471d-cb93-d66b51d1f7df"
      },
      "source": [
        "%%time\n",
        "documents = [_text.split() for _text in df_train.review] "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 452 ms, sys: 186 ms, total: 638 ms\n",
            "Wall time: 638 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsT03LDJGpOG"
      },
      "source": [
        "import gensim\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n",
        "                                            window=W2V_WINDOW, \n",
        "                                            min_count=W2V_MIN_COUNT, \n",
        "                                            workers=8)\n",
        "w2v_model.build_vocab(documents)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yowCP2eyGwQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83031cd7-bfc9-47c1-b884-01557fa89941"
      },
      "source": [
        "words = w2v_model.wv.vocab.keys()\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size 17836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VYUHGt4Gzk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5b2779-197a-4b3b-9299-6cbc1bebdf46"
      },
      "source": [
        "%%time\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 15min 5s, sys: 3.9 s, total: 15min 8s\n",
            "Wall time: 7min 45s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(148837349, 169839264)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJcHr_wbG3PS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529ec4c6-be65-45df-9011-fb6c9dfab385"
      },
      "source": [
        "%%time\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train.review)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words 64128\n",
            "CPU times: user 5.14 s, sys: 44 ms, total: 5.18 s\n",
            "Wall time: 5.21 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kO3fgIhG7bk"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "SEQUENCE_LENGTH = 300\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.review), maxlen=SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.review), maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKjRkfdZG_Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea5d876-265d-4f8f-f5a3-4e22d91c6803"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train.sentiment.tolist())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyTtbmCwHDCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a895ae32-050a-4f05-9920-67c12dd9075d"
      },
      "source": [
        "y_train = encoder.transform(df_train.sentiment.tolist())\n",
        "y_test = encoder.transform(df_test.sentiment.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)\n",
        "import numpy as np\n",
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train (40000, 1)\n",
            "y_test (10000, 1)\n",
            "(64128, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oopezP58HO6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1674c1-bd9b-4995-ed6d-161cccdb52bc"
      },
      "source": [
        "from __future__ import absolute_import,division,print_function\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tqdm\n",
        "import tqdm.auto\n",
        "tqdm.tqdm=tqdm.auto.tqdm\n",
        "print(tf.__version__)\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "#tf.debugging.set_log_device_placement(False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHuX05PpHSZ_"
      },
      "source": [
        "xtrain=tf.constant(x_train)\n",
        "ytrain=tf.constant(y_train)\n",
        "xtest=tf.constant(x_test)\n",
        "ytest=tf.constant(y_test)\n",
        "max_words=300"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcTY12YLHdYQ"
      },
      "source": [
        "model6=tf.keras.Sequential([\n",
        "                           tf.keras.layers.Embedding(vocab_size,100,\n",
        "                                                     #mask_zero=True\n",
        "                                                     input_length=max_words\n",
        "                                                     ),\n",
        "                           tf.keras.layers.Dropout(0.2),\n",
        "                           tf.keras.layers.Bidirectional(\n",
        "                               tf.keras.layers.GRU(128,#dropout=0.4,\n",
        "                                               return_sequences=True)\n",
        "                           ),                            \n",
        "                           tf.keras.layers.Conv1D(32,kernel_size=3,\n",
        "                                                  padding='same',\n",
        "                                                  activation=tf.nn.relu),\n",
        "                           tf.keras.layers.GlobalMaxPooling1D(),\n",
        "                           tf.keras.layers.Dense(64,activation=tf.nn.relu),  \n",
        "                           tf.keras.layers.Dropout(0.5),\n",
        "                           tf.keras.layers.Dense(1,activation=tf.nn.sigmoid),\n",
        "])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY1dRc6YHgPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ccb749-3b07-4d66-f26a-79e970861777"
      },
      "source": [
        "model6.compile(#optimizer='adam',\n",
        "              tf.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model6.fit(xtrain,ytrain,\n",
        "          validation_data=(xtest,ytest),\n",
        "          epochs=3,\n",
        "          batch_size=128,\n",
        "          #train_dataset,\n",
        "          verbose=1,\n",
        "          #callbacks = [early_stop]\n",
        "          )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "313/313 [==============================] - 676s 2s/step - loss: 0.3827 - accuracy: 0.8184 - val_loss: 0.2552 - val_accuracy: 0.8956\n",
            "Epoch 2/3\n",
            "313/313 [==============================] - 668s 2s/step - loss: 0.1884 - accuracy: 0.9316 - val_loss: 0.2463 - val_accuracy: 0.9020\n",
            "Epoch 3/3\n",
            "313/313 [==============================] - 647s 2s/step - loss: 0.1120 - accuracy: 0.9615 - val_loss: 0.2841 - val_accuracy: 0.8940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f996ce72dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgJhpLu6Hy6m"
      },
      "source": [
        "import time\n",
        "def predict(text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    # Tokenize text\n",
        "    text=text.replace(\"\\n\", \"\")\n",
        "    text=preprocess(text)\n",
        "    if len(text)==0:\n",
        "      return 300\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    #feature_tensor = torch.from_numpy(x_test)\n",
        "    feature_tensor = tf.constant(x_test)\n",
        "    pred=model6.predict_classes(feature_tensor,verbose=1)\n",
        "    #pred = torch.round(output.squeeze()\n",
        "    #return pred.item()\n",
        "    return pred[0][0]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utjJQjhh0tKK",
        "outputId": "f27aa36a-55a8-46a0-fbd7-7428dda96190"
      },
      "source": [
        "!unzip /content/9e9afdaabac711ea.zip"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/9e9afdaabac711ea.zip\n",
            "   creating: Data Files/\n",
            "   creating: Data Files/Dataset/\n",
            "  inflating: Data Files/Dataset/Test903.jpg  \n",
            "  inflating: Data Files/Dataset/Test2434.jpg  \n",
            "  inflating: Data Files/Dataset/Test145.jpg  \n",
            "  inflating: Data Files/Dataset/Test1615.jpg  \n",
            "  inflating: Data Files/Dataset/Test1122.jpg  \n",
            "  inflating: Data Files/Dataset/Test132.jpg  \n",
            "  inflating: Data Files/Dataset/Test2959.jpg  \n",
            "  inflating: Data Files/Dataset/Test109.jpg  \n",
            "  inflating: Data Files/Dataset/Test114.jpg  \n",
            "  inflating: Data Files/Dataset/Test230.jpg  \n",
            "  inflating: Data Files/Dataset/Test536.jpg  \n",
            "  inflating: Data Files/Dataset/Test516.jpg  \n",
            "  inflating: Data Files/Dataset/Test233.jpg  \n",
            "  inflating: Data Files/Dataset/Test2792.jpg  \n",
            "  inflating: Data Files/Dataset/Test119.jpg  \n",
            "  inflating: Data Files/Dataset/Test2616.jpg  \n",
            "  inflating: Data Files/Dataset/Test1934.jpg  \n",
            "  inflating: Data Files/Dataset/Test803.jpg  \n",
            "  inflating: Data Files/Dataset/Test3030.jpg  \n",
            "  inflating: Data Files/Dataset/Test240.jpg  \n",
            "  inflating: Data Files/Dataset/Test107.jpg  \n",
            "  inflating: Data Files/Dataset/Test212.jpg  \n",
            "  inflating: Data Files/Dataset/Test2870.jpg  \n",
            "  inflating: Data Files/Dataset/Test757.jpg  \n",
            "  inflating: Data Files/Dataset/Test245.jpg  \n",
            "  inflating: Data Files/Dataset/Test243.jpg  \n",
            "  inflating: Data Files/Dataset/Test1923.jpg  \n",
            "  inflating: Data Files/Dataset/Test3177.jpg  \n",
            "  inflating: Data Files/Dataset/Test103.jpg  \n",
            "  inflating: Data Files/Dataset/Test482.jpg  \n",
            "  inflating: Data Files/Dataset/Test890.jpg  \n",
            "  inflating: Data Files/Dataset/Test372.jpg  \n",
            "  inflating: Data Files/Dataset/Test2648.jpg  \n",
            "  inflating: Data Files/Dataset/Test1353.jpg  \n",
            "  inflating: Data Files/Dataset/Test812.jpg  \n",
            "  inflating: Data Files/Dataset/Test1271.jpg  \n",
            "  inflating: Data Files/Dataset/Test523.jpg  \n",
            "  inflating: Data Files/Dataset/Test129.jpg  \n",
            "  inflating: Data Files/Dataset/Test140.jpg  \n",
            "  inflating: Data Files/Dataset/Test581.jpg  \n",
            "  inflating: Data Files/Dataset/Test353.jpg  \n",
            "  inflating: Data Files/Dataset/Test1012.jpg  \n",
            "  inflating: Data Files/Dataset/Test779.jpg  \n",
            "  inflating: Data Files/Dataset/Test141.jpg  \n",
            "  inflating: Data Files/Dataset/Test179.jpg  \n",
            "  inflating: Data Files/Dataset/Test113.jpg  \n",
            "  inflating: Data Files/Dataset/Test1359.jpg  \n",
            "  inflating: Data Files/Dataset/Test162.jpg  \n",
            "  inflating: Data Files/Dataset/Test198.jpg  \n",
            "  inflating: Data Files/Dataset/Test151.jpg  \n",
            "  inflating: Data Files/Dataset/Test1022.jpg  \n",
            "  inflating: Data Files/Dataset/Test125.jpg  \n",
            "  inflating: Data Files/Dataset/Test449.jpg  \n",
            "  inflating: Data Files/Dataset/Test213.jpg  \n",
            "  inflating: Data Files/Dataset/Test2235.jpg  \n",
            "  inflating: Data Files/Dataset/Test789.jpg  \n",
            "  inflating: Data Files/Dataset/Test833.jpg  \n",
            "  inflating: Data Files/Dataset/Test1279.jpg  \n",
            "  inflating: Data Files/Dataset/Test533.jpg  \n",
            "  inflating: Data Files/Dataset/Test105.jpg  \n",
            "  inflating: Data Files/Dataset/Test134.jpg  \n",
            "  inflating: Data Files/Dataset/Test3706.jpg  \n",
            "  inflating: Data Files/Dataset/Test3625.jpg  \n",
            "  inflating: Data Files/Dataset/Test172.jpg  \n",
            "  inflating: Data Files/Dataset/Test1803.jpg  \n",
            "  inflating: Data Files/Dataset/Test128.jpg  \n",
            "  inflating: Data Files/Dataset/Test135.jpg  \n",
            "  inflating: Data Files/Dataset/Test236.jpg  \n",
            "  inflating: Data Files/Dataset/Test192.jpg  \n",
            "  inflating: Data Files/Dataset/Test1785.jpg  \n",
            "  inflating: Data Files/Dataset/Test489.jpg  \n",
            "  inflating: Data Files/Dataset/Test1644.jpg  \n",
            "  inflating: Data Files/Dataset/Test238.jpg  \n",
            "  inflating: Data Files/Dataset/Test2280.jpg  \n",
            "  inflating: Data Files/Dataset/Test929.jpg  \n",
            "  inflating: Data Files/Dataset/Test545.jpg  \n",
            "  inflating: Data Files/Dataset/Test225.jpg  \n",
            "  inflating: Data Files/Dataset/Test136.jpg  \n",
            "  inflating: Data Files/Dataset/Test912.jpg  \n",
            "  inflating: Data Files/Dataset/Test1290.jpg  \n",
            "  inflating: Data Files/Dataset/Test117.jpg  \n",
            "  inflating: Data Files/Dataset/Test490.jpg  \n",
            "  inflating: Data Files/Dataset/Test1767.jpg  \n",
            "  inflating: Data Files/Dataset/Test144.jpg  \n",
            "  inflating: Data Files/Dataset/Test2590.jpg  \n",
            "  inflating: Data Files/Dataset/Test2209.jpg  \n",
            "  inflating: Data Files/Dataset/Test211.jpg  \n",
            "  inflating: Data Files/Dataset/Test378.jpg  \n",
            "  inflating: Data Files/Dataset/Test173.jpg  \n",
            "  inflating: Data Files/Dataset/Test941.jpg  \n",
            "  inflating: Data Files/Dataset/Test188.jpg  \n",
            "  inflating: Data Files/Dataset/Test1837.jpg  \n",
            "  inflating: Data Files/Dataset/Test2650.jpg  \n",
            "  inflating: Data Files/Dataset/Test191.jpg  \n",
            "  inflating: Data Files/Dataset/Test3525.jpg  \n",
            "  inflating: Data Files/Dataset/Test149.jpg  \n",
            "  inflating: Data Files/Dataset/Test131.jpg  \n",
            "  inflating: Data Files/Dataset/Test498.jpg  \n",
            "  inflating: Data Files/Dataset/Test360.jpg  \n",
            "  inflating: Data Files/Dataset/Test3277.jpg  \n",
            "  inflating: Data Files/Dataset/Test161.jpg  \n",
            "  inflating: Data Files/Dataset/Test1884.jpg  \n",
            "  inflating: Data Files/Dataset/Test751.jpg  \n",
            "  inflating: Data Files/Dataset/Test668.jpg  \n",
            "  inflating: Data Files/Dataset/Test606.jpg  \n",
            "  inflating: Data Files/Dataset/Test242.jpg  \n",
            "  inflating: Data Files/Dataset/Test1885.jpg  \n",
            "  inflating: Data Files/Dataset/Test993.jpg  \n",
            "  inflating: Data Files/Dataset/Test126.jpg  \n",
            "  inflating: Data Files/Dataset/Test2455.jpg  \n",
            "  inflating: Data Files/Dataset/Test979.jpg  \n",
            "  inflating: Data Files/Dataset/Test824.jpg  \n",
            "  inflating: Data Files/Dataset/Test187.jpg  \n",
            "  inflating: Data Files/Dataset/Test1001.jpg  \n",
            "  inflating: Data Files/Dataset/Test1889.jpg  \n",
            "  inflating: Data Files/Dataset/Test218.jpg  \n",
            "  inflating: Data Files/Dataset/Test1634.jpg  \n",
            "  inflating: Data Files/Dataset/Test228.jpg  \n",
            "  inflating: Data Files/Dataset/Test3159.jpg  \n",
            "  inflating: Data Files/Dataset/Test2791.jpg  \n",
            "  inflating: Data Files/Dataset/Test3294.jpg  \n",
            "  inflating: Data Files/Dataset/Test164.jpg  \n",
            "  inflating: Data Files/Dataset/Test244.jpg  \n",
            "  inflating: Data Files/Dataset/Test237.jpg  \n",
            "  inflating: Data Files/Dataset/Test3893.jpg  \n",
            "  inflating: Data Files/Dataset/Test100.jpg  \n",
            "  inflating: Data Files/Dataset/Test526.jpg  \n",
            "  inflating: Data Files/Dataset/Test614.jpg  \n",
            "  inflating: Data Files/Dataset/Test3024.jpg  \n",
            "  inflating: Data Files/Dataset/Test3934.jpg  \n",
            "  inflating: Data Files/Dataset/Test227.jpg  \n",
            "  inflating: Data Files/Dataset/Test811.jpg  \n",
            "  inflating: Data Files/Dataset/Test3749.jpg  \n",
            "  inflating: Data Files/Dataset/Test122.jpg  \n",
            "  inflating: Data Files/Dataset/Test396.jpg  \n",
            "  inflating: Data Files/Dataset/Test204.jpg  \n",
            "  inflating: Data Files/Dataset/Test835.jpg  \n",
            "  inflating: Data Files/Dataset/Test239.jpg  \n",
            "  inflating: Data Files/Dataset/Test397.jpg  \n",
            "  inflating: Data Files/Dataset/Test176.jpg  \n",
            "  inflating: Data Files/Dataset/Test3798.jpg  \n",
            "  inflating: Data Files/Dataset/Test181.jpg  \n",
            "  inflating: Data Files/Dataset/Test180.jpg  \n",
            "  inflating: Data Files/Dataset/Test945.jpg  \n",
            "  inflating: Data Files/Dataset/Test3130.jpg  \n",
            "  inflating: Data Files/Dataset/Test2007.jpg  \n",
            "  inflating: Data Files/Dataset/Test168.jpg  \n",
            "  inflating: Data Files/Dataset/Test843.jpg  \n",
            "  inflating: Data Files/Dataset/Test2309.jpg  \n",
            "  inflating: Data Files/Dataset/Test677.jpg  \n",
            "  inflating: Data Files/Dataset/Test884.jpg  \n",
            "  inflating: Data Files/Dataset/Test209.jpg  \n",
            "  inflating: Data Files/Dataset/Test859.jpg  \n",
            "  inflating: Data Files/Dataset/Test778.jpg  \n",
            "  inflating: Data Files/Dataset/Test2743.jpg  \n",
            "  inflating: Data Files/Dataset/Test1240.jpg  \n",
            "  inflating: Data Files/Dataset/Test861.jpg  \n",
            "  inflating: Data Files/Dataset/Test624.jpg  \n",
            "  inflating: Data Files/Dataset/Test1818.jpg  \n",
            "  inflating: Data Files/Dataset/Test206.jpg  \n",
            "  inflating: Data Files/Dataset/Test519.jpg  \n",
            "  inflating: Data Files/Dataset/Test374.jpg  \n",
            "  inflating: Data Files/Dataset/Test3360.jpg  \n",
            "  inflating: Data Files/Dataset/Test2424.jpg  \n",
            "  inflating: Data Files/Dataset/Test837.jpg  \n",
            "  inflating: Data Files/Dataset/Test440.jpg  \n",
            "  inflating: Data Files/Dataset/Test3750.jpg  \n",
            "  inflating: Data Files/Dataset/Test221.jpg  \n",
            "  inflating: Data Files/Dataset/Test522.jpg  \n",
            "  inflating: Data Files/Dataset/Test234.jpg  \n",
            "  inflating: Data Files/Dataset/Test143.jpg  \n",
            "  inflating: Data Files/Dataset/Test2962.jpg  \n",
            "  inflating: Data Files/Dataset/Test644.jpg  \n",
            "  inflating: Data Files/Dataset/Test2049.jpg  \n",
            "  inflating: Data Files/Dataset/Test3863.jpg  \n",
            "  inflating: Data Files/Dataset/Test3789.jpg  \n",
            "  inflating: Data Files/Dataset/Test1071.jpg  \n",
            "  inflating: Data Files/Dataset/Test957.jpg  \n",
            "  inflating: Data Files/Dataset/Test165.jpg  \n",
            "  inflating: Data Files/Dataset/Test612.jpg  \n",
            "  inflating: Data Files/Dataset/Test1743.jpg  \n",
            "  inflating: Data Files/Dataset/Test400.jpg  \n",
            "  inflating: Data Files/Dataset/Test3118.jpg  \n",
            "  inflating: Data Files/Dataset/Test3827.jpg  \n",
            "  inflating: Data Files/Dataset/Test1776.jpg  \n",
            "  inflating: Data Files/Dataset/Test231.jpg  \n",
            "  inflating: Data Files/Dataset/Test661.jpg  \n",
            "  inflating: Data Files/Dataset/Test108.jpg  \n",
            "  inflating: Data Files/Dataset/Test760.jpg  \n",
            "  inflating: Data Files/Dataset/Test1992.jpg  \n",
            "  inflating: Data Files/Dataset/Test942.jpg  \n",
            "  inflating: Data Files/Dataset/Test1229.jpg  \n",
            "  inflating: Data Files/Dataset/Test3512.jpg  \n",
            "  inflating: Data Files/Dataset/Test1902.jpg  \n",
            "  inflating: Data Files/Dataset/Test183.jpg  \n",
            "  inflating: Data Files/Dataset/Test2114.jpg  \n",
            "  inflating: Data Files/Dataset/Test1872.jpg  \n",
            "  inflating: Data Files/Dataset/Test250.jpg  \n",
            "  inflating: Data Files/Dataset/Test3975.jpg  \n",
            "  inflating: Data Files/Dataset/Test527.jpg  \n",
            "  inflating: Data Files/Dataset/Test761.jpg  \n",
            "  inflating: Data Files/Dataset/Test1620.jpg  \n",
            "  inflating: Data Files/Dataset/Test2411.jpg  \n",
            "  inflating: Data Files/Dataset/Test834.jpg  \n",
            "  inflating: Data Files/Dataset/Test660.jpg  \n",
            "  inflating: Data Files/Dataset/Test133.jpg  \n",
            "  inflating: Data Files/Dataset/Test160.jpg  \n",
            "  inflating: Data Files/Dataset/Test158.jpg  \n",
            "  inflating: Data Files/Dataset/Test194.jpg  \n",
            "  inflating: Data Files/Dataset/Test1717.jpg  \n",
            "  inflating: Data Files/Dataset/Test216.jpg  \n",
            "  inflating: Data Files/Dataset/Test2068.jpg  \n",
            "  inflating: Data Files/Dataset/Test946.jpg  \n",
            "  inflating: Data Files/Dataset/Test2371.jpg  \n",
            "  inflating: Data Files/Dataset/Test1856.jpg  \n",
            "  inflating: Data Files/Dataset/Test2977.jpg  \n",
            "  inflating: Data Files/Dataset/Test232.jpg  \n",
            "  inflating: Data Files/Dataset/Test665.jpg  \n",
            "  inflating: Data Files/Dataset/Test1199.jpg  \n",
            "  inflating: Data Files/Dataset/Test3864.jpg  \n",
            "  inflating: Data Files/Dataset/Test587.jpg  \n",
            "  inflating: Data Files/Dataset/Test166.jpg  \n",
            "  inflating: Data Files/Dataset/Test3985.jpg  \n",
            "  inflating: Data Files/Dataset/Test154.jpg  \n",
            "  inflating: Data Files/Dataset/Test715.jpg  \n",
            "  inflating: Data Files/Dataset/Test3265.jpg  \n",
            "  inflating: Data Files/Dataset/Test152.jpg  \n",
            "  inflating: Data Files/Dataset/Test1724.jpg  \n",
            "  inflating: Data Files/Dataset/Test1954.jpg  \n",
            "  inflating: Data Files/Dataset/Test249.jpg  \n",
            "  inflating: Data Files/Dataset/Test937.jpg  \n",
            "  inflating: Data Files/Dataset/Test417.jpg  \n",
            "  inflating: Data Files/Dataset/Test201.jpg  \n",
            "  inflating: Data Files/Dataset/Test1161.jpg  \n",
            "  inflating: Data Files/Dataset/Test3566.jpg  \n",
            "  inflating: Data Files/Dataset/Test579.jpg  \n",
            "  inflating: Data Files/Dataset/Test422.jpg  \n",
            "  inflating: Data Files/Dataset/Test3246.jpg  \n",
            "  inflating: Data Files/Dataset/Test1788.jpg  \n",
            "   creating: Data Files/Sample Data Files/\n",
            "  inflating: Data Files/Sample Data Files/Sample_Negative.jpg  \n",
            "  inflating: Data Files/Sample Data Files/Sample_Positive.jpg  \n",
            "  inflating: Data Files/Sample Data Files/Sample_Random.jpg  \n",
            "  inflating: Data Files/Sample_Submission.csv  \n",
            "  inflating: Data Files/Test.csv     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USy3EG-BH4ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34827890-0dfd-42dc-a850-731b50a96152"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "import pytesseract\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "df1=pd.read_csv('Data Files/Test.csv')\n",
        "m=df1['Filename'].tolist()\n",
        "mh=[]\n",
        "for i in m:\n",
        "  c='Data Files/Dataset/'+i\n",
        "  img=cv2.imread(c)\n",
        "  #img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "  aa=pytesseract.image_to_string(img)\n",
        "  mh.append(aa)\n",
        "  #cv2_imshow(img) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 1s (3,523 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.8-py2.py3-none-any.whl size=14071 sha256=8b6dc0e8c3edd43143bd062de33440151964c43a68aeb9a144225978ffaa1c1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/89/b9/3f11250225d0f90e5454fcc30fd1b7208db226850715aa9ace\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgPpsg3FH93B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9876ff-418d-4d75-c0af-24c601fafb8e"
      },
      "source": [
        "labe=['']*len(mh)\n",
        "co=0\n",
        "#rot=[300]*len(mh)\n",
        "for i in range(len(mh)):\n",
        "  if len(mh[i])==0:\n",
        "    labe[i]='Random'\n",
        "    co+=1\n",
        "    continue\n",
        "  blob=predict(mh[i])\n",
        "  print(blob)\n",
        "\n",
        "  #po=blob.sentiment.polarity\n",
        "  if blob==300:\n",
        "    labe[i]='Random'\n",
        "    co+=1\n",
        "    continue\n",
        "  if blob==1:\n",
        "    labe[i]='Positive'\n",
        "  else:\n",
        "    labe[i]='Negative'"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 662ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1\n",
            "300\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "0\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "300\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "0\n",
            "300\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1bpyFfuIGBG"
      },
      "source": [
        "df1['Category']=labe\n",
        "df1.to_csv('result41.csv',index=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKsPRJCz4xGb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}